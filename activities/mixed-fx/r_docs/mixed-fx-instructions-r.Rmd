---
title: "Mixed Effects activity in R, based on Brown (2021)"
author: "Lobue & Bhanji - Statistical Methods"
date: "12/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
library(tidyverse)
library(ggplot2)
library(lme4)
```

------------------------------------------------------------------------

Today's activity is based entirely on [Violet Brown's 2021 article, "An Introduction to Linear Mixed-Effects Modeling in R." Advances in Methods and Practices in Psychological Science, 4(1), pp. 1–19.](https://doi.org/10.1177%2F2515245920960351)  

## Goals for today  

- Learn differences between a linear mixed effects model approach and a standard linear model approach with aggregated data for repeated measures in a crossed design  
- Learn about fixed and random effects and how to specify them in a formula using the `lme4::lmer()` function in R  
- Learn how to address problems in model fitting (failure to converge, or singular fit)  
- Learn how to interpret output from a linear mixed effects model  
- Use model comparison for significance tests of individual terms in a model  
- Learn to extend a simple two condition comparison to examine a 2x2 design, and interpret the lower order effects along with the interaction effect   

------------------------------------------------------------------------

### Step 0  

- install the required packages: 
    - `install.packages("lme4")`  
    - `afex` you should already have installed  
- setup your project folder (with data and r_docs folders within) and start a new project in this existing folder  
- start a new R markdown for your code and notes and include the usual code in the setup chunk, but add `library(lme4)` to the setup chunk (and then run the chunk)   
- Download these files and place in the data folder within your project folder:  
    - [rt_dummy_data.csv](../data/rt_dummy_data.csv) for example 1  

    
------------------------------------------------------------------------
## Example 1: trial level analysis of a within-subject (2 level) effect  

###  Step 1 - import the first data file and check it out  
Data description:  
This data set is adapted from a study investigating comprehension of spoken words based on audio alone (Audio-only condition) vs audio with video of the speaker (Audiovisual condition). On each of 553 trials, participants heard and repeated a single word (either Audio-only or Audiovisual) while simultaneously performing a second task (judging the duration of a vibrating stimulus on their knee). The researchers hypothesized that response times for the secondary task would be longer in the Audiovisual condition than the Audio-only condition, which would indicate greater listening effort required in the Audiovisual condition. This finding is relevant to theories of speech processing.   

Variables:  
- **Dependent variable:** `RT`=response time (ms) for the secondary task  
- **Independent variable:** `modality`=listening condition ("Audio-only" or "Audiovisual")  
- `stim`=word stimulus for each trial  
- `PID`=participant identifier  

Each of 53 participants complete 553 trials. The response times have been modified as described in Brown (2012) to illustrate issues involved in linear mixed effects modeling.  

#### What to do:  
1. Use the `readr::read_csv()` function to import the `rt_dummy_data.csv` file and store it in a variable named `rt_data`. There are no missing data that you need to deal with, because trials with a missed response are not included in the data file. Store the `modality` variable as a factor.  

2. Use the `dplyr::group_by()` and `dplyr::summarise()` functions to compute the following and print it to the screen:
    - mean RT across all trials in each modality condition  
    - median RT across all trials in each modality condition 
    - standard devation of RT across all trials in each modality condition  

3. Now, first aggregate the data by participant (calculate the number of trials each participant did of each condition, and the mean RT for each condition for each subject) and then print out:  
    - mean number of trials in each condition for a participant
    - min and max number of trials in each condition for a participant  
    - mean RT across participants (different than before bc you are first aggregating by participant)  
    - standard deviation of RT across participants  

<button class="btn btn-primary" data-toggle="collapse" data-target="#step1"> Show/Hide Solution </button>  
<div id="step1" class="collapse">  
```{r import-rt-data}
#import the data
rt_data <- readr::read_csv("data/rt_dummy_data.csv") %>% mutate(
  modality = forcats::as_factor(modality)
)

#descriptives across trials (ignoring participant ID)
rt_data %>% dplyr::group_by(modality) %>% dplyr::summarise(
  totalN = n(),
  trialwiseRTmean = mean(RT, na.rm = TRUE),
  trialwiseRTmedian = median(RT, na.rm = TRUE),
  trialwiseRTsd = sd(RT, na.rm = TRUE)
) %>% ungroup() %>% 
  kableExtra::kbl(caption = "descriptives by modality condition (trialwise)",
                  digits = 2) %>% 
  kableExtra::kable_classic(full_width=FALSE, lightable_options = "hover")

#calc number of trials and mean RT for each participant (by condition)
rt_bysub <- rt_data %>% dplyr::group_by(modality,PID) %>% dplyr::summarise(
  bysub_N = n(),
  bysub_RT = mean(RT)
) %>% ungroup()

# now print descriptives, calculated by first aggregating by subject
rt_bysub %>% dplyr::group_by(modality) %>% dplyr::summarise(
  bysub_mean_N = mean(bysub_N),
  bysub_min_N = min(bysub_N),
  bysub_max_N = max(bysub_N),
  bysub_mean_RT = mean(bysub_RT),
  bysub_sd_RT = sd(bysub_RT)
) %>% ungroup() %>% 
  kableExtra::kbl(caption = "descriptives, first aggregated by participant", 
                  digits = 2) %>% 
  kableExtra::kable_classic(full_width=FALSE, lightable_options = "hover")

# histogram and box plot of the trialwise data
rt_data %>% ggplot( aes(x=RT, fill=modality) ) +
  geom_histogram(position = "identity", alpha = .5, binwidth = 100) +
  theme_classic() + labs(title = "trialwise RTs - histogram")
rt_data %>% ggplot( aes(x=modality, y=RT) ) +
  geom_boxplot() + theme_classic() + labs(title = "trialwise RTs - box plot")

# histogram and box plot of the trialwise data
rt_bysub %>% ggplot( aes(x=bysub_RT, fill=modality) ) +
  geom_histogram(position = "identity", alpha = .5, binwidth = 100) +
  theme_classic() + labs(title = "RTs averaged by participant")
rt_bysub %>% ggplot( aes(x=modality, y=bysub_RT) ) +
  geom_boxplot() + theme_classic() + labs(title = "RTs averaged by participant - box plot")


```
</div>
&nbsp;

- in the trialwise data averaged by condition you should see that mean RT in the Audio-only condition is 1041 ms and in the Audiovisual condition it is 1125 ms  
- in the data that is first averaged by participant and then across participants, the mean RT in the Audio-only condition is 1044 ms and in the Audiovisual condition it is 1127 ms
- the difference in means calculated in the two different ways is due to different numbers of trials for each participant and condition in the trialwise calculation (look at the min and max trial numbers), but in the second calculation each participant's average RTs are weighted equally. The difference in the two calculations is small here but can be larger in datasets with larger imbalances.  
- You should also see that the data are positively skewed (whether we look at trialwise data or aggregated by subject) - but we have a sufficient sample size so we won't worry about it.  

------------------------------------------------------------------------

### Step 2: Use a traditional method to analyze aggregated RTs (by participant or by item)  

#### Step 2.1 - Model the data after aggregating by participant    

First, let's use each participant's average RT per condition as our unit of analysis. Once we average across trials to get a two values (Audio-only mean RT, Audio-visual mean RT) for each participant, then we can just do a paired t-test on those values like we have done before.  
Try it now, you can use the tibble of RTs averaged by subject and modality that you created in the step above (called `rt_bysub` in the solution code), and pipe it to the `t.test()` function that we've used before.    

<button class="btn btn-primary" data-toggle="collapse" data-target="#step21"> Show/Hide Solution </button>  
<div id="step21" class="collapse">  
```{r t-test}
# paired t-test with mean RT per subject in each condition
rt_bysub %>% dplyr::arrange(PID) %>% t.test(bysub_RT ~ modality, data = ., paired=TRUE)

# equivalently, you can run the same test with the lmer function. the formula will make
# more sense after doing the whole activity
rt_bysub_lmer <- rt_bysub %>% lme4::lmer(formula = bysub_RT ~ modality + 1 + (1|PID))
summary(rt_bysub_lmer)
```
</div>
&nbsp;

- The mean response time in the Audiovisual condition is 83ms slower than in the Audio-only condition. The t-statistic (t(52) = -6.612) and low p-value (.000000021) show that it is unlikely to observe sample means this different if there is no true difference.   
- This is a simple and commonly used approach to analyze data with many trials per condition, where the first step is to average over all trials in a condition for each participant (referred to as *by-participant analysis* in [Judd, Westfall, and Kenny (2012)](https://doi.org/10.1037/a0028347)). 
- today we're going over an alternate approach, linear mixed modeling, where the unit of analysis can be individual trials. Some advantages of this approach from [Brown (2021)](https://doi.org/10.1177%2F2515245920960351) and [Judd, Westfall, and Kenny (2012)](https://doi.org/10.1037/a0028347):  
    - captures variance (e.g. trial-to-trial) that may be important for accurately estimating effects of interest  
    - allows modeling of multiple random variables (e.g., participants and stimuli), whereas ANOVA approaches allow only one random variable    
    - handles unbalanced designs (and missing data)  
    - handles categorical or continuous predictors, coefficients give magnitude and direction of effects (as opposed to ANOVA approach)  
    - extends to other types of outcome variables (e.g., binary)

Next, we will use a linear mixed model approach where trial RTs are the outcome variable, and the model allows for varying intercepts for each participant (e.g. some participants may tend to respond slower overall) and varying intercepts for each word stimulus (e.g., some words may tend to have longer response times). The model also allows for a varying effect of modality in different subjects and in different stimuli.     

------------------------------------------------------------------------

### Step 3: Linear mixed-effects model of the effect of modality on trial response time  

##### Reminder: what are fixed and random effects?  
- `modality` is systematically varied so we consider it a fixed effect. In designs (e.g., correlation) where no variable is systematically manipulated we can consider a fixed effect to be one where all levels of interest are exhaustively covered (e.g., `age` in the lumosity data we used long ago)  
- participants are a sample of a larger population so we consider `PID` a random effect  
- words used in the study are a sample from a larger set so we consider `stim` a random effect  

[Barr et al (2013)](https://doi.org/10.1016/j.jml.2012.11.001) advise using the maximal random effects structure justified by the design. In this case we are justified to include the full random effect structure.

#### Full random effect structure  
- by-participant varying intercepts (1|PID)  
- by-stimulus varying intercepts (1|stim)  
- by-participant varying effect of modality, aka random slope (modality|PID)  
- by-stimulus varying effect of modality, aka random slope (modality|stim)  

So let's specify the model in a typical R formula syntax. Just the fixed effects part would be `RT ~ 1 + modality` to specify that `RT` is the DV and it is predicted by `modality` plus an intercept term. **Remember that `lm()` will automatically dummy code (aka treatment coding) a factor variable. The function for linear mixed models, `lmer()`, does the same. So entering `modality` as a predictor is equivalent to a variable where "Audio-only" is set to 0 and "Audiovisual" is set to 1.**  

So we start with the formula `RT ~ 1 + modality`
- Then we add the by-participant varying intercepts like this: `RT ~ 1 + modality + (1 |PID)`  
- add the by-stimulus varying intercepts: `RT ~ 1 + modality + (1|PID) + (1|stim)`  
- add the by-participant varying effect of modality (random slope): `RT ~ 1 + modality + (1 + modality|PID) + (1|stim)`  
- add the by-stimulus varying effect of modality: `RT ~ 1 + modality + (1 + modality|PID) + (1 + modality|stim)`  

#### Step 3.1 - Fit the model  
Okay, we have the model formula. We will use the `lme4::lmer()` function to estimate the model - you call it in basically the same way that we have used the `lm()` function before. Pass the data and the formula to the function and store the result in a variable (let's call it `rt_full_mod`). Then pass that variable to the `summary()` function. 

<button class="btn btn-primary" data-toggle="collapse" data-target="#step31"> Show/Hide Solution </button>  
<div id="step31" class="collapse">  
```{r step3.1}
rt_full_mod <- lmer(RT ~ 1 + modality + (1 + modality|PID) + (1 + modality|stim), 
                    data = rt_data)
summary(rt_full_mod)
```
</div>
&nbsp;

Did you get a message that the "model failed to converge"? The message means that the algorithm used to estimate the model parameters could not find a good fit within the allotted number of iterations. Although the `summary()` function gives reasonable looking parameters in this case, **you should not report results of a model that failed to converge** - the nonconvergence means the model has not been reliably estimated.  

#### What to do when a model does not converge  
Here are strategies covered in [Brown (2021)](https://doi.org/10.1177%2F2515245920960351):
1. Recheck your model and make sure it is not misspecified  
2. Are there major imbalances in the data? (e.g., if one participant or item has very few observations it can cause nonconvergence - consider dropping the participant/item)  
3. Adjust the parameters that control the model fitting process (e.g., change the optimizer method, increase iterations allotted)  
4. Simplify the model by removing correlation between random effects (see [Brown (2021)](https://doi.org/10.1177%2F2515245920960351) p. 10)  
5. Last resort, consider a simpler random effects structure - but document this decision fully, the random effects structure should be theoretically motivated so this is a compromise.  

#### Step 3.2 - use afex::all_fit to test different optimizers  
We will try changing the optimizer (strategy 3). First, use the `afex::all_fit()` function (include only the model we fit above as an argument). This function will try several different optimizer algorithms and report warning messages for each. Then you can pick one optimizer that converges and use it. Click the button for the code to run the `all_fit()` function and see the output:

<button class="btn btn-primary" data-toggle="collapse" data-target="#step32"> Show/Hide Solution </button>  
<div id="step32" class="collapse">  
```{r step3.2}
afex::all_fit(rt_full_mod)
```
</div>
&nbsp;

#### Step 3.3 - Re-fit the model with a different optimizer  
The output of `afex::all_fit()` shows us that the "bobyqa", "Nelder_Mead", and "optimx.nlminb" methods all converge (others do not). When we first ran the model above, by default `lmer()` used the "nloptwrap" method.  
So we will use one of the converging methods in its place now - let's use "bobyqa". We do this by passing an extra argument to `lmer()` called `control` with the value set to be `control = lmerControl(optimizer = "bobyqa"). Then call `summary()` like before 

<button class="btn btn-primary" data-toggle="collapse" data-target="#step33"> Show/Hide Solution </button>  
<div id="step33" class="collapse">  
```{r step3.3}
rt_full_mod <- lmer(RT ~ 1 + modality + (1 + modality|PID) + (1 + modality|stim), 
                    data = rt_data, 
                    control = lmerControl(optimizer = "bobyqa"))
summary(rt_full_mod)
```
</div>
&nbsp;

#### Step 3.4 - hypothesis testing    
The first thing you may notice in the output is that there is no p-value for our fixed effect. The recommended way to test a null hypothesis is by comparing a model with the effect of interest to a model that is identical but with the effect of interest removed. Our effect of interest is the fixed effect of modality, so we'll remove it in the reduced model (but leave the random effects as is, including modality random slopes). Let's do that now. First, fit a model called `rt_modalityremoved_mod`, then use the `anova()` function to compare it to the full model with a likelihood ratio test. Use the same parameters to fit the reduced model that were used for the full model.  

<button class="btn btn-primary" data-toggle="collapse" data-target="#step34"> Show/Hide Solution </button>  
<div id="step34" class="collapse">  
```{r}
rt_modalityremoved_mod <- lmer(RT ~ 1 + (1 + modality|PID) + (1 + modality|stim), 
                    data = rt_data, 
                    control = lmerControl(optimizer = "bobyqa"))
anova(rt_modalityremoved_mod,rt_full_mod)
summary(rt_full_mod)
```
</div>
&nbsp;

### Step 4 - Interpret the output of the model comparison and the full model summary  

##### Model comparison (output of `anova()`):
- the "Chisq" value of 32.385 and low p-value (.00000001264) indicate that the full model (including the fixed effect of modality) fits better than the reduced model, so we can reject the null hypothesis that modality has no effect on response time.

##### Full model summary (output of `summary()` for the full model):
- The parameter estimate for the effect of modality is 83.18, meaning that a 1 unit increase in modality predicts an RT increase of 83.18 ms. In other words, participants were 83 ms slower in the audiovisual (coded as 1) relative to the audio-only (coded as 0) condition.  
- The fixed intercept is 1044.14, meaning that when `modality` is 0 (i.e., in the Audio-only condition) the mean response time is 1044.14  
- standard error estimates how much the coefficient varies across samples  
- the t-value is the coefficient divided by its standard error  

We could report the results like this (from [Brown (2021)](https://doi.org/10.1177%2F2515245920960351)):  
> "A linear mixed effects model was fit to trial response times, with a fixed effect of modality, and random effects structure allowing intercepts and slopes of the modality effect to vary by-participant and by-item. A likelihood-ratio test indicated that the model including the fixed effect of modality provided a better fit for the data than a model without it, χ<sup>2</sup>(1) = 32.39, p < .001. Examination of the summary output for the full model indicated that response times were on average an estimated 83 ms slower in the audiovisual relative to the audio-only condition(β = 83.18, SE = 12.58, t = 6.62)."

That's probably enough for most purposes but you can also take a closer look at individual stimuli and participants by looking at individual participant and item intercept and slope estimates by using the `coef()` function (you will get over 500 lines of output)
<button class="btn btn-primary" data-toggle="collapse" data-target="#step4"> Show/Hide Code </button>  
<div id="step4" class="collapse">  
```{r step4}
coef(rt_full_mod)
```
</div>
&nbsp;

If you are interested in seeing more examples, you can find Violet Brown's R markdown document for this data at [https://osf.io/v6qag/](https://osf.io/v6qag/).

### That's all for this activity!